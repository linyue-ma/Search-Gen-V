# Nugget Matching Evaluation Framework (v2.0)

A unified, configurable evaluation framework for nugget matching models in the search-gen-v pipeline. This framework eliminates code duplication, provides flexible configuration management, and supports both single-run and multi-run statistical evaluation with comprehensive error analysis.

## âœ¨ Key Features

- **ğŸ”§ Zero Hardcoding**: All configuration externalized to YAML files
- **ğŸ¯ Unified CLI**: Single entry point for all evaluation modes
- **ğŸ“Š Dual Metrics System**: Performance analysis on both valid and all predictions
- **ğŸ“ˆ Statistical Analysis**: Multi-run evaluation with 95% confidence intervals
- **ğŸ¨ Multi-Format Support**: 10+ output formats with intelligent parsing
- **ğŸ”„ Sequential Recovery**: Preserves partial predictions from truncated responses
- **ğŸ“ Complete Logging**: Separate evaluation and LLM call logs with run_id correlation
- **âš¡ Parallel Processing**: Configurable worker processes
- **ğŸ”Œ Extensible Design**: Easy to add new metrics and evaluation modes
- **ğŸ” Advanced Analytics**: Comprehensive case study and interactive analysis tools
- **ğŸ›¡ï¸ System Validation**: Built-in health checks and log correlation verification
- **ğŸ›ï¸ Interactive Exploration**: Command-line interface for real-time data analysis

## ğŸ†• **v2.0 Highlights: Dual Metrics & Enhanced Analysis**

### **Dual Metrics System**
- **Valid Predictions**: Metrics calculated only on successfully parsed predictions
- **All Predictions**: Comprehensive metrics including error/None predictions as special categories
- **Complete Comparison**: See both ideal performance and real-world impact of parsing failures

### **Enhanced Multi-Format Support**
- **10+ Formats**: JSON, CSV, Markdown, YAML, XML, TSV, Numbered lists, Pipe-separated, etc.
- **Sequential Parsing**: Intelligent format detection with fixed priority order
- **Partial Recovery**: Preserve valid predictions from partially failed parsing attempts

### **Advanced Statistical Analysis**
- **Extended Metrics**: pass@k, avg@k, maj@k at both nugget and sample levels
- **Confidence Intervals**: 95% CI for all metrics in multi-run mode
- **Error Breakdown**: Detailed analysis of truncation, format errors, and count mismatches

### **ğŸ†• Run ID System & Log Correlation**
- **Unified Tracking**: Every evaluation run gets a unique run_id for complete traceability
- **Log Correlation**: Automatic correlation between evaluation logs and LLM API call logs
- **System Validation**: Built-in health checks for log integrity and run_id consistency
- **Enhanced Diagnostics**: Improved error handling and resource management

## ğŸš€ Quick Start

### 1. Environment Setup
```bash
# Install uv if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Setup the environment
./setup.sh
source .venv/bin/activate
```

### 2. Generate Configuration
```bash
# Generate configuration templates
nugget-eval --generate-config config/my_thinking.yaml --template-type thinking
nugget-eval --generate-config config/my_multi.yaml --template-type multi_run
```

### 3. Configure Your Evaluation
Edit your configuration file:
```yaml
model:
  base_url: "http://localhost:8000/v1"
  name: "/path/to/your/model"
  format_type: "adaptive"        # Controls prompt format
  error_handling: "sequential"   # Preserve partial results
  enable_thinking: true

data:
  input_path: "/path/to/input.jsonl"
  gold_path: "/path/to/gold.jsonl"

evaluation:
  num_runs: 1          # Single run (1) or multi-run (16+)
  batch_size: 10
  num_workers: 8
```

### 4. Run Evaluation
```bash
# Single-run evaluation
nugget-eval --config config/my_thinking.yaml

# Multi-run statistical analysis  
nugget-eval --config config/my_multi.yaml --num-runs 16
```

## ğŸ“Š Output Examples

### Single-Run Dual Metrics
```
--- Nugget-level Match Evaluation ---
  Total Predictions: 1250
  Valid Predictions: 1180 (94.4%)
  Error Predictions: 45
  None Predictions: 25

  Performance Metrics (on valid predictions):
    Micro Accuracy:  0.8542
    Micro Precision: 0.8542
    Micro Recall:    0.8542
    Micro F1 Score:  0.8542

  Performance Metrics (on all predictions):
    Micro Accuracy:  0.8063    # Lower due to errors
    Micro Precision: 0.8063
    Micro Recall:    0.8063
    Micro F1 Score:  0.8063

  Batch Analysis:
    Success Rate: 84.0%
    Partial Success Rate: 12.0%
    Overall Recovery Rate: 96.8%
    
    Format Usage:
      json           : 67 batches (53.6%)
      markdown       : 35 batches (28.0%)
      csv            : 15 batches (12.0%)
```

### Multi-Run Statistical Analysis
```
--- Multi-Run Statistics (16 runs) (Nugget-Level) ---

  Metric Statistics (on valid predictions):
    Micro F1: Mean = 0.8542, Std = 0.0124, 95% CI = (0.8418, 0.8666)
    Macro F1: Mean = 0.8523, Std = 0.0156, 95% CI = (0.8367, 0.8679)

  Metric Statistics (on all predictions):
    Micro F1: Mean = 0.7234, Std = 0.0178, 95% CI = (0.7056, 0.7412)
    Macro F1: Mean = 0.7156, Std = 0.0189, 95% CI = (0.6967, 0.7345)

--- Nugget-Level Extended Statistics ---
  Pass@K (at least one correct):
    pass@1: 0.7245    pass@5: 0.8967    pass@10: 0.9234
  
  Avg@K (average accuracy):
    avg@1: 0.7245     avg@5: 0.8123     avg@10: 0.8234
  
  Maj@K (majority voting):
    maj@3: 0.8456     maj@5: 0.8567     maj@7: 0.8634

--- Sample-Level Extended Statistics ---
  Pass@K: pass@1: 0.6234, pass@5: 0.8456, pass@10: 0.9012
  Avg@K:  avg@1: 0.6234,  avg@5: 0.7345,  avg@10: 0.7678
  Maj@K:  maj@3: 0.7234,  maj@5: 0.7456,  maj@7: 0.7634
```

## ğŸ”§ Configuration Reference

### Model Configuration
```yaml
model:
  base_url: "http://localhost:8000/v1"
  api_key: "EMPTY"
  name: "/path/to/model"
  temperature: 0.7
  top_p: 0.95
  max_tokens: 1024
  enable_thinking: true
  
  # Multi-format output configuration
  format_type: "adaptive"          # adaptive|json|markdown|csv|yaml|xml|etc.
  error_handling: "sequential"     # sequential|strict
  partial_recovery: true           # Enable partial prediction recovery
```

### Evaluation Configuration
```yaml
evaluation:
  batch_size: 10          # Nuggets per batch
  num_workers: 8          # Parallel workers
  num_runs: 1            # Single run (1) or multi-run (16+)

metrics:
  avg_k: [1, 5, 10]      # avg@k calculation
  pass_k: [1, 5, 10]     # pass@k calculation
  maj_k: [3, 5, 7]       # maj@k calculation (use odd numbers)
```

### Logging Configuration
```yaml
logging:
  eval_log_dir: "logs/eval"
  llm_log_dir: "logs/llm"
  save_predictions: true
  log_level: "INFO"
  
  # Enhanced reporting options
  report_detail_level: "full"      # full|standard|minimal
  show_format_analysis: true       # Show format usage statistics
  show_error_breakdown: true       # Show detailed error analysis
  show_batch_recovery_stats: true  # Show partial recovery stats
```

## ğŸ“ Project Structure

```
Search-Gen-V/evaluator/
â”œâ”€â”€ eval/                    # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py               # CLI interface
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ common.py            # Multi-format parsing & model client
â”‚   â”œâ”€â”€ evaluator.py         # Unified evaluator with dual metrics
â”‚   â”œâ”€â”€ metrics.py           # Enhanced metrics calculation
â”‚   â”œâ”€â”€ exceptions.py        # Custom exceptions
â”‚   â”œâ”€â”€ constants.py         # ğŸ†• System constants and configuration
â”‚   â””â”€â”€ validators.py        # ğŸ†• Run ID and log validation utilities
â”œâ”€â”€ config/                   # Configuration templates
â”‚   â”œâ”€â”€ base.yaml           # Base template
â”‚   â”œâ”€â”€ thinking_mode.yaml  # Single-run with thinking
â”‚   â”œâ”€â”€ multi_run.yaml      # Multi-run statistical
â”‚   â””â”€â”€ demo_v2.yaml        # v2.0 feature demo
â”œâ”€â”€ logs/                    # Output logs (auto-created)
â”‚   â”œâ”€â”€ eval/              # Evaluation logs (organized by run_id)
â”‚   â”‚   â””â”€â”€ <run_id>/      # ğŸ†• Per-run log isolation
â”‚   â””â”€â”€ llm/               # LLM call logs (organized by run_id)
â”‚       â””â”€â”€ <run_id>/      # ğŸ†• Per-run log isolation
â”œâ”€â”€ tests/                   # ğŸ†• Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_run_id_system.py
â”œâ”€â”€ case_study.py            # ğŸ†• Advanced analysis and reporting tool
â”œâ”€â”€ interactive_analysis.py  # ğŸ†• Interactive command-line analysis
â”œâ”€â”€ run_tests.py            # ğŸ†• Test runner
â”œâ”€â”€ predictions_to_gold.py   # Prediction format converter
â”œâ”€â”€ setup.sh                # Environment setup script
â”œâ”€â”€ run_eval.sh            # Convenient evaluation runner
â”œâ”€â”€ pyproject.toml         # Package configuration
â”œâ”€â”€ IMPROVEMENTS_SUMMARY.md  # ğŸ†• Detailed improvement documentation
â””â”€â”€ README.md
```

## ğŸ¯ Usage Examples

### Format-Specific Testing
```bash
# Test markdown-only output
nugget-eval --config config/base.yaml --format-type markdown

# Test JSON output with strict error handling
nugget-eval --config config/base.yaml --format-type json --error-handling strict
```

### Statistical Analysis
```bash
# Quick 5-run analysis
nugget-eval --config config/multi_run.yaml --num-runs 5

# Comprehensive 20-run analysis with extended metrics
nugget-eval --config config/multi_run.yaml --num-runs 20
```

### Parameter Overrides
```bash
nugget-eval --config config/base.yaml \
  --input-path /new/data.jsonl \
  --batch-size 5 \
  --num-runs 10 \
  --format-type adaptive \
  --report-detail-level full
```

### ğŸ†• Advanced Analysis Tools

#### Case Study Analysis
Perform comprehensive analysis of evaluation runs with automatic log correlation:

```bash
# Analyze a specific run in detail
python case_study.py --run-id 20240101_120000_abc12345 --log-dir logs/ --detailed

# Compare multiple runs
python case_study.py --compare-runs 20240101_120000_abc12345 20240101_130000_def67890 --log-dir logs/

# Analyze all available runs for trends
python case_study.py --analyze-all --log-dir logs/ --limit 10

# Generate HTML reports
python case_study.py --run-id 20240101_120000_abc12345 --log-dir logs/ --format html --save-report

# Perform system health check
python case_study.py --health-check --log-dir logs/

# List all available runs
python case_study.py --list-runs --log-dir logs/
```

#### Interactive Analysis
Launch an interactive command-line interface for real-time exploration:

```bash
# Start interactive analyzer
python interactive_analysis.py --log-dir logs/

# Available interactive commands:
analysis> list_runs                    # List available runs
analysis> load_run 20240101_120000_abc12345  # Load specific run
analysis> show_summary detailed        # Show detailed run summary
analysis> show_errors 10              # Show first 10 errors
analysis> query_qid Q12345            # Analyze specific question
analysis> search "timeout"            # Search across loaded data
analysis> compare 20240101_130000_def67890  # Compare with another run
analysis> export summary my_report.json     # Export analysis
analysis> health_check                 # Check system health
analysis> help                         # Show all commands
```

#### System Validation
Validate run_id system integrity and log correlation:

```bash
# Run comprehensive tests
python run_tests.py -v

# Quick health check
python -c "from nugget_eval.validators import validate_system_health; print(validate_system_health('logs/eval', 'logs/llm'))"

# Validate specific run_id format
python -c "from nugget_eval.validators import validate_run_id; print(validate_run_id('20240101_120000_abc12345'))"
```

### Convert eval logs to gold/input JSONL

å½“ä½ é€šè¿‡ `nugget-eval` å¾—åˆ°é€ nugget çš„è¯„ä¼°æ—¥å¿—ï¼ˆæ¯è¡Œå¿…é¡»å« `qid`,`query`,`block_text`,`nugget_text`,`block_pred`,`block_true`ï¼‰åï¼Œå¯ä»¥ï¼š

1) ç”Ÿæˆ goldï¼ˆå¯é€‰ä» input æ¨¡æ¿æ³¨å…¥ `query` å¹¶æŒ‰æ¨¡æ¿é¡ºåºè¾“å‡º nuggetsï¼‰ï¼š

```bash
python predictions_to_gold.py \
  --eval-log logs/eval/20250101_120000_single_run_predictions.jsonl \
  --input-template /wuxi_gpfs/user/malinyue/data/release/eval/rag24.jsonl \
  --output-gold /wuxi_gpfs/user/malinyue/data/release/eval/rag24sample.jsonl \
  --fallback not_support
```

2) åŸºäºç°æœ‰ input æ¨¡æ¿å›å¡«/è¦†ç›– assignment ç”Ÿæˆæ–°çš„ inputï¼š

```bash
python predictions_to_gold.py \
  --eval-log logs/eval/20250101_120000_single_run_predictions.jsonl \
  --input-template /wuxi_gpfs/user/malinyue/data/release/eval/rag24.jsonl \
  --output-input /wuxi_gpfs/user/malinyue/data/release/eval/rag24.updated.jsonl \
  --fallback not_support
```

å¦‚æœå¸Œæœ›å¯¹äº eval ç¼ºå¤±çš„ nuggets ä¿ç•™æ¨¡æ¿ä¸­çš„åŸ assignment è€Œä¸æ˜¯å†™å…¥ fallbackï¼š

```bash
python predictions_to_gold.py \
  --eval-log logs/eval/20250101_120000_single_run_predictions.jsonl \
  --input-template /wuxi_gpfs/user/malinyue/data/release/eval/rag24.jsonl \
  --output-input /wuxi_gpfs/user/malinyue/data/release/eval/rag24.updated.jsonl \
  --keep-existing-when-missing
```

#### æ ‡å‡†æ ¼å¼ä¸ä¸¥æ ¼åŒ¹é…è¯´æ˜ï¼ˆé‡è¦ï¼‰

- è¾“å…¥æ¨¡æ¿ä¸­ï¼Œ`block` å­—æ®µä¸ºï¼š
  - `List[Any]`ï¼Œå…¶ä¸­ `block[0]` æ˜¯ passage æ–‡æœ¬ï¼Œ`block[1]` æ˜¯ `List[{"title": str, "url": str}]`ï¼ˆå‚è€ƒåˆ—è¡¨ï¼‰ã€‚
  - æˆ–å…¼å®¹ `str`ï¼ˆä»… passage æ–‡æœ¬ï¼‰ã€‚
- è¯„ä¼°æ—¥å¿—ï¼ˆpredictions JSONLï¼‰ä¸­è‹¥å­˜åœ¨ `block_text`ï¼Œè„šæœ¬ä¼šå¯¹åŒä¸€ `qid` ä¸‹æ‰§è¡Œ per-block èšåˆï¼Œå¹¶åœ¨æ›´æ–° input æ—¶â€œä¸¥æ ¼ä½¿ç”¨â€æ¨¡æ¿çš„ `block[0]` ä¸ `block_text` åšç²¾ç¡®åŒ¹é…ï¼š
  - è‹¥è¯¥ `qid` å­˜åœ¨ per-block èšåˆä½†æ— æ³•åŒ¹é…æ¨¡æ¿çš„ `block[0]`ï¼Œè„šæœ¬å°†ç›´æ¥æŠ¥é”™ï¼ˆä¸å›é€€ï¼‰ã€‚
  - è‹¥è¯¥ `qid` ä¸å­˜åœ¨ per-block èšåˆï¼Œåˆ™å›é€€ä¸º qid çº§åˆ«çš„å…¨å±€èšåˆã€‚
- gold è¾“å‡ºä¸º per-qid çš„ `global_nuggets_assignment`ï¼ˆå¯å« `query`ï¼‰ï¼Œä¸åŒ…å« per-block `blocks` å­—æ®µã€‚
 - è¯„ä¼°æ—¥å¿—æ¯æ¡è®°å½•å¿…é¡»åŒ…å« `query` ä¸ `block_text`ï¼›ç¼ºå¤±å°†æŠ¥é”™ã€‚æ¯ä¸ª `qid` çš„ `query` å¿…é¡»èƒ½ä»æ¨¡æ¿æˆ–è¯„ä¼°æ—¥å¿—è¡¥é½ï¼Œå¦åˆ™æŠ¥é”™ã€‚

ç¤ºä¾‹å‘½ä»¤ï¼ˆä¸¥æ ¼åŒ¹é…ä¸ gold è¾“å‡ºå¯¹é½æ ‡å‡†ï¼‰ï¼š

```bash
python predictions_to_gold.py \
  --eval-log logs/eval/<run_id>/single_run_predictions.jsonl \
  --input-template /path/to/rag24.jsonl \
  --output-gold /path/to/rag24_gold.jsonl \
  --output-input /path/to/rag24_input.updated.jsonl \
  --fallback not_support
```

è¯´æ˜ï¼š
- `assignment` èšåˆè§„åˆ™ä¸ºæœ€å¤§æ”¯æŒåº¦ä¼˜å…ˆï¼šsupport > partial_support > not_supportã€‚
- èšåˆä¼šå¿½ç•¥ `None` ä¸ `error`ï¼›è‹¥å…¨éƒ¨æ— æ•ˆï¼Œåˆ™ä½¿ç”¨ `--fallback`ï¼ˆé»˜è®¤ `not_support`ï¼‰ã€‚
- gold è¾“å‡ºæŒ‰ `qid` åˆ†ç»„ï¼›gold è¡ŒåŒ…å« `query` å­—æ®µï¼ˆè‹¥æ¨¡æ¿ç¼ºå¤±åˆ™ä»è¯„ä¼°æ—¥å¿—è·å–ï¼›æ— æ³•è·å–å°†æŠ¥é”™ï¼‰ï¼ŒåŒæ—¶ nuggets é¡ºåºå°†æŒ‰æ¨¡æ¿çš„ `block_nuggets_assignment.text` æ’åºï¼›å¦åˆ™æŒ‰å­—å…¸åºã€‚
- input æ›´æ–°ä»…è¦†ç›– `block_nuggets_assignment[].assignment`ï¼Œä¿ç•™ `docids`ã€`importance`ã€`block`ã€`query` ç­‰å…¶ä½™å­—æ®µä¸ç»“æ„ã€‚

## ğŸ“ˆ Key Metrics Explained

### **Dual Metrics System**
- **Valid Predictions**: Calculated only on successfully parsed outputs
- **All Predictions**: Include None/error as special failure categories
- **Gap Analysis**: Difference shows impact of parsing failures

### **Extended Statistics** (Multi-run only)
- **pass@k**: Probability that â‰¥1 of k runs succeeds
- **avg@k**: Average accuracy over k runs  
- **maj@k**: Majority voting accuracy over k runs
- **Nugget-Level**: Traditional per-nugget analysis
- **Sample-Level**: Per-query complete correctness analysis

### **Error Analysis**
- **Batch Status**: success/partial_success/complete_failure rates
- **Format Usage**: Which formats models actually use
- **Recovery Rate**: Data saved from partial parsing failures
- **Error Types**: Truncation, format errors, count mismatches

## ğŸš¨ Migration Notes

### From v1.0 Legacy Scripts
```bash
# Old way
python eval_with_thinking.py
python eval_without_thinking.py

# New way (fully compatible)
nugget-eval --config config/thinking_mode.yaml
nugget-eval --config config/multi_run.yaml
```

### Key Changes in v2.0
1. **Dual metrics**: Both valid and all predictions analyzed
2. **Simplified parsing**: Removed fallback_formats configuration
3. **Enhanced reporting**: More detailed error analysis
4. **Extended statistics**: Sample-level pass@k/avg@k/maj@k added

## ğŸ’¡ Best Practices

### Evaluation Best Practices
1. **Use multi-run for production**: `num_runs >= 16` for reliable statistics
2. **Monitor format compliance**: Check format usage in batch analysis  
3. **Analyze error patterns**: Use detailed error breakdown for model improvement
4. **Compare dual metrics**: Gap indicates parsing robustness issues
5. **Use odd numbers for maj@k**: Avoid ties in majority voting

### ğŸ†• Analysis and Debugging Best Practices
6. **Start with health checks**: Always run `python case_study.py --health-check` before analysis
7. **Use interactive mode for exploration**: `interactive_analysis.py` for real-time investigation
8. **Correlate logs systematically**: Each run_id automatically correlates eval and LLM logs
9. **Track performance trends**: Use `--analyze-all` to identify performance degradation
10. **Export findings for sharing**: Save analysis reports for team collaboration

### Workflow Recommendations
```bash
# Recommended analysis workflow
1. nugget-eval --config your_config.yaml           # Run evaluation
2. python case_study.py --health-check --log-dir logs/  # Verify system health
3. python interactive_analysis.py --log-dir logs/       # Explore results
4. python case_study.py --analyze-all --log-dir logs/   # Generate trends report
```

### Troubleshooting Guide
- **API errors**: Use `query_qid <qid>` to trace specific failures
- **Performance issues**: Compare runs with `case_study.py --compare-runs`
- **System problems**: Check `validate_system_health()` output
- **Log correlation issues**: Verify run_id consistency with health checks

## ğŸ¯ é€‚ç”¨åœºæ™¯ä¸é€‰æ‹©æŒ‡å—

### Case Study Analysis é€‚ç”¨äºï¼š
- **æ€§èƒ½è°ƒä¼˜**: æ·±å…¥åˆ†æç‰¹å®šrunçš„æ€§èƒ½ç“¶é¢ˆå’Œä¼˜åŒ–æœºä¼š
- **é”™è¯¯è¯Šæ–­**: ç³»ç»ŸåŒ–åˆ†æAPIé”™è¯¯ã€è§£æå¤±è´¥ç­‰é—®é¢˜
- **trendåˆ†æ**: è·Ÿè¸ªå¤šä¸ªrunä¹‹é—´çš„æ€§èƒ½å˜åŒ–è¶‹åŠ¿
- **æŠ¥å‘Šç”Ÿæˆ**: ä¸ºå›¢é˜Ÿæˆ–å®¢æˆ·ç”Ÿæˆä¸“ä¸šçš„åˆ†ææŠ¥å‘Š
- **ç³»ç»Ÿå¥åº·ç›‘æ§**: å®šæœŸæ£€æŸ¥æ—¥å¿—ç³»ç»Ÿçš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§

### Interactive Analysis é€‚ç”¨äºï¼š
- **å®æ—¶è°ƒè¯•**: åœ¨å¼€å‘è¿‡ç¨‹ä¸­å¿«é€Ÿå®šä½å’Œåˆ†æé—®é¢˜
- **æ•°æ®æ¢ç´¢**: äº¤äº’å¼æŸ¥è¯¢ç‰¹å®šQIDæˆ–é”™è¯¯æ¨¡å¼
- **ä¸´æ—¶åˆ†æ**: æ— éœ€ç¼–å†™è„šæœ¬å³å¯è¿›è¡Œå¤æ‚çš„æ•°æ®æŸ¥è¯¢
- **å­¦ä¹ ç³»ç»Ÿ**: ç†Ÿæ‚‰æ•°æ®ç»“æ„å’Œè¯„ä¼°ç»“æœçš„åˆ†å¸ƒ
- **å¿«é€ŸéªŒè¯**: éªŒè¯ä¿®å¤æ•ˆæœæˆ–é…ç½®å˜æ›´çš„å½±å“

### å·¥å…·é€‰æ‹©å»ºè®®ï¼š
```
å®šæœŸç›‘æ§ â†’ case_study.py --health-check
æ·±åº¦åˆ†æ â†’ case_study.py --analyze-all  
é—®é¢˜è°ƒè¯• â†’ interactive_analysis.py
æŠ¥å‘Šç”Ÿæˆ â†’ case_study.py --save-report
ç³»ç»ŸéªŒè¯ â†’ run_tests.py
```

---

## ğŸš€ æ–°åŠŸèƒ½æ€»ç»“

**v2.1 æ–°å¢åŠŸèƒ½ï¼ˆåŸºäºrun_idç³»ç»Ÿï¼‰:**
- ğŸ”— **å®Œæ•´çš„æ—¥å¿—å…³è”ç³»ç»Ÿ**: é€šè¿‡run_idå®ç°eval_logå’Œllm_logçš„è‡ªåŠ¨å…³è”
- ğŸ“Š **é«˜çº§åˆ†æå·¥å…·**: case_study.pyæä¾›ä¸“ä¸šçº§çš„æ€§èƒ½å’Œé”™è¯¯åˆ†æ
- ğŸ›ï¸ **äº¤äº’å¼æ¢ç´¢ç•Œé¢**: interactive_analysis.pyæ”¯æŒå®æ—¶æ•°æ®æŸ¥è¯¢å’Œåˆ†æ
- ğŸ›¡ï¸ **ç³»ç»ŸéªŒè¯æ¡†æ¶**: å†…ç½®å¥åº·æ£€æŸ¥å’Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶
- ğŸ“ˆ **è¶‹åŠ¿åˆ†æèƒ½åŠ›**: è·¨å¤šä¸ªrunçš„æ€§èƒ½è¶‹åŠ¿å’Œæ¨¡å¼è¯†åˆ«
- ğŸ” **ç²¾ç¡®é—®é¢˜å®šä½**: QIDçº§åˆ«çš„è¿½è¸ªå’Œé”™è¯¯ä¸Šä¸‹æ–‡åˆ†æ

è¿™äº›å·¥å…·å°†evaluation frameworkä»åŸºç¡€çš„æŒ‡æ ‡è®¡ç®—å‡çº§ä¸ºå®Œæ•´çš„æ¨¡å‹æ€§èƒ½åˆ†æå’Œè°ƒä¼˜å¹³å°ã€‚

---

**v2.1 delivers comprehensive evaluation with dual metrics analysis and advanced analytics tools, making it easy to understand both model capabilities and real-world deployment impact with complete traceability.**